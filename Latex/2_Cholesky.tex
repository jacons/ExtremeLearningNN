\subsection{Direct method with Cholesky factorization}
In the second algorithm we change slightly the notation. We denote the input data $X \in \mathbb{R}^{f \times n}$ where $f$ are the number of feature and $n$ is number of example, of course $f << n$. And $Y \in \mathbb{R}^{o \times n}$ where $o$ is the output dimension. Talking about the direct method, we observe the problem from another point of view. We want find the $W_2$ that minimize the difference between $W_2\sigma(W_1X)$ and $Y$ (in norm), more precisely, we formulate the problem into following expression:

\begin{equation}
\label{formula:LS}
\min_{W_2} \|W_2\sigma(W_1X) - Y\|_2^2  + \alpha^2\|W_2\|_2
\end{equation}

For simplicity we will use the following equation knowing that ($H = \sigma(W_1X)$) and $H \in \mathbb{R}^{h \times n}$. $h$ is the hidden dimension of the second layer and $h << n$.

\begin{equation}
\label{formula:LS}
\min_{W_2} \|W_2H- Y\|_2^2  + \alpha^2\|W_2\|_2
\end{equation}


The second term of (Eq. \ref{formula:LS}) is the $L_2$ regularization \cite{ridge_reg}. Intuitively, it aims to keep low (small values) $W_2$ in order to avoid over-fitting and improve the generalization of the model. However, in the following sections, we will explore the behavior of the model changing the $\alpha$ parameter. The least-square problem can be solved in plenty of methods, but in our project, we are focused into one single approach: the Cholesky factorization (or decomposition) method.

First of all we need to define some useful notions:
\begin{definition}
    \label{def:a_trasposea}
    For each $A \in \mathbb{R}^{ a \times b}$ where $a <= b$, $(AA^T)$ is symmetric and positive semi-definite and $(AA^T) \in \mathbb{R}^{ a \times a}$
\end{definition}
\begin{theorem}
    \label{th:th_choleksy}
    Each matrix $A \in \mathbb{R}^{ n \times n}$ positive definite ($A \succ 0$) can be always decomposed in $ A = R^TR$ where $R \in \mathbb{R}^{n \times n}$ upper triangular. This decomposition is called Cholesky factorization.
\end{theorem} 


From (Def. \ref{def:a_trasposea}) we can say that $HH^T \succ 0$ thus we can apply the (Th. \ref{th:th_choleksy}). Thus $HH^T = R^TR$ where $R^T \in \mathbb{R}^{h \times h}$.

\begin{equation}
    \label{def:gradient_cholesky}
    \begin{aligned}
        \min_{W_2} \|W_2H - Y\|_2^2 = \min_{W_2} \|W_2HH^T- YH^T\|_2^2 = \min_{W_2} \|W_2R^TR- YH^T\|_2^2\\
    \end{aligned}
\end{equation}

For simplicity we set $R^TR = C$. We know that $C^T = C$ because $(R^TR)^T = R^TR$.

\begin{equation}
    \label{def:gradient_cholesky}
    \begin{aligned}
        \min_{W_2} \|W_2C- YH^T\|_2^2 &= \\
        &= \min_{W_2} (W_2C - YH^T)^T(W_2C - YH^T)\\
        &= \min_{W_2} (W_2C)^TW_2C - (W_2C)^TYH^T - (YH^T)^TW_2C + (YH^T)^TYH^T \\
        &= \min_{W_2} CW_2^TW_2C - CW_2^TYH^T + HY^TW_2C + HY^TYH^T \\ 
        &= \min_{W_2} CW_2^TW_2C - 2CW_2^TYH^T + HY^TYH^T \\ 
    \end{aligned}
\end{equation}

We also observe that $CW_2^TYH^T$ and $HY^TW_2C$ are equal. The gradient of (Eq. \ref{def:gradient_cholesky}):

\begin{equation}
    \begin{aligned}
       \nabla \|W_2H- Y\|_2^2 = 2CW_2C - 2CYH^T 
    \end{aligned}
\end{equation}

The (local)minimum of the function is when the gradient is equal to 0 :

\begin{equation}
    \begin{aligned}
        2CW_2C = 2CYH^T
    \end{aligned}
\end{equation}

We multiply both side by $C^{-1}$ and divide by 2. $C$ is always invertible since is a square matrix.

\begin{equation}
    \begin{aligned}
        2C^{-1}CW_2C = 2C^{-1}CYH^T \\
    \end{aligned}
\end{equation}

Finally we obtain:

\begin{equation}
    \begin{aligned}
        W_2C = YH^T \equiv W_2R^TR = YH^T \\
    \end{aligned}
\end{equation}

To find $W_2$ we solve the following linear system:

\begin{equation}
\label{formula:linearsyscholesyk}
\systeme{z = W_2R^T, zR = YH^T}
\end{equation}


We can notice that the second equation in \ref{formula:linearsyscholesyk} is composed by $R \in \mathbb{R}^{h \times h} $  upper triangular and $YH^T \in \mathbb{R}^{o \times h}$ thus we can easily find $z$ by backward substitution. 
Once found $z$ we can apply the forward substitution on the first equation to finally find $W_2$.

\subsubsection{Complexity}
To analyze the complexity, we have to take into consideration all operations that are involved. In the table (Tab. \ref{table:complexity}) we show which are the steps and their relative complexity.

\begingroup
    \renewcommand{\arraystretch}{2} % Default value: 1
    \begin{table}[!h]
        \begin{center}
            \begin{tabular}{ |c|c|c| } 
                \hline
                \textbf{Step} & \textbf{Operation} & \textbf{Complexity} \\ 
                \hline\hline
                1 & $HH^T$ & $h^2n$ \\ 
                \hline
                2 & $yH^T$ & $hno$ \\ 
                \hline
                3 & $R^TR = HH^T$ & $\frac{1}{3}h^3 + \mathcal{O}(h^2)$ \\ 
                \hline
                4 &$zR = YH^T$ & $\mathcal{O}(h^2o)$ \\ 
                \hline
                5 & $W_2R^T = z$ & $\mathcal{O}(h^2o)$ \\ 
                \hline
            \end{tabular}
            \caption{Operation involved in the direct method with Cholesky.}
            \label{table:complexity}
        \end{center}
    \end{table}
\endgroup
As we can notice, we most time consuming part is the Cholesky decomposition (step 3). We can conclude that the algorithm has the complexity equal to $\mathcal{O}(h^3)$.

\subsubsection{Stability}