\subsection{(A1) Accelerated Gradient method}

\subsubsection{Introduction}

%what are accelerated gradient method
Accelerated Gradient Methods (ACM) are a class of optimization techniques used to solve optimization problems, to find the minimum of multivariate functions. 
Accelerated gradient methods are most effective when dealing with smooth and potentially convex objective functions.
Non-convex functions can also benefit from accelerated gradient methods, but the presence of multiple local minima and saddle points may make convergence to a global minimum challenging.
These methods are designed to converge faster than traditional gradient descent algorithms by incorporating momentum-like terms to navigate the optimization landscape more efficiently.

ACM are iterative methods like \textit{Heavy Ball} \cite{heavyball} gradient methods, but they differ in how they incorporate momentum and update the parameters.
The Heavy Ball Method uses a simple update rule, which is a combination of the negative gradient direction and a fraction of the previous update direction (momentum), we can see the update rule in (Eq. \ref{eq:updateruleheavyball}), where $\alpha$ is the step size, $\beta$ is the momentum term, and $\nabla f(x^{(k)})$ is the gradient of the objective function at point $x^{(k)}$. Note that $\alpha$ and $\beta$ changes at each iteration but they can be fixed.

\begin{equation}
\label{eq:updateruleheavyball}
x^{(k+1)} = x^{(k)} - \alpha^{(k)} \nabla f(x^{(k)}) + \beta^{(k)}(x^{(k)} - x^{(k-1)})
\end{equation}


ACM instead, uses a more sophisticated update rule. It calculates the gradient at an intermediate point, which we can call a "look-ahead" point, and then uses this information to update the current position. 
We can see the update rule of ACM in (Eq. \ref{eq:updateruleacm}). 
Here, $y^{(k+1)}$ is the look-ahead point, and $x^{(k+1)}$ is the updated position. ACM uses the gradient information at the look-ahead point to anticipate the gradient direction and update $x^{(k)}$ accordingly. In particular, in \ref{alg:ACG} we can see the pseudo-code of the algorithm. 

\begin{equation}
\label{eq:updateruleacm}
\begin{aligned}
y^{(k+1)} &= x^{(k)} + \beta (x^{(k)} - x^{(k-1)}) \\
x^{(k+1)} &= y^{(k+1)} -\alpha \nabla f(y^{(k+1)})    
\end{aligned}
\end{equation}

\begin{algorithm}
\caption{Pseudocode of Accelerated Gradient Method (ACM)}
\label{alg:ACG}
\begin{algorithmic}[1]
\STATE Initialize $x^{(0)}$ \COMMENT{Initial parameter vector}
\STATE Initialize $y^{(0)} = x^{(0)}$ \COMMENT{Initialize look-ahead point}
\STATE Initialize step size $\alpha$ and  momentum parameter $\beta$
\STATE Set the maximum number of iterations $T$
\STATE Set accuracy $\epsilon$
\STATE k = 0
\WHILE{ ( $k < T$ and $|| \nabla f(x) || > \epsilon $ )}
    \STATE Update $y^{(k+1)} = x^{(k)} + \beta (x^{(k)} - x^{(k-1)}) $ \COMMENT{Look-ahead step}
    \STATE Update $x^{(k+1)} = y^{(k+1)} -\alpha \nabla f(y^{(k+1)})$ \COMMENT{Actual update with anti-gradient}
    \STATE k = k+1
\ENDWHILE
\RETURN Final parameter vector $x^{(T)}$
\end{algorithmic}
\end{algorithm}

\todo[inline]{Risolvere lo =0 molesto}

Regarding our tasks, the objective function that we want to minimize is the Loss function (Eq. \ref{eq:loss}), where $y$ is the expected output and $\hat{y}$ is the output of the model. The error function is defined in (Eq.\ref{eq:error}) and it is the sum of squared error, the last term is the L2 regularization term as defined in \cite{ridge_reg}.  We want to learn our model parameters $\theta = \{\textbf{w}\}$, minimizing the loss and updating the parameters accordingly to the update rule specified above. 

\begin{equation}
\label{eq:loss}
Loss = Error(y,\hat{y}) + \lambda \sum_{n=1}^{N} w_i^2
\end{equation}

\begin{equation}
\label{eq:error}
Error(y, \hat{y}) = \sum_{n=1}^{N}(y_i - \hat{y}_i)^2 = || y - \hat{y} ||_2^2
\end{equation}


\subsubsection{Convergence Properties}
First of all, we must specify a few definitions that will be useful in explaining the complexity of the algorithm. In particular, we say:

\todo[inline]{tutte queste definizioni si leggono male, nel caso cambiare stile}
\begin{definition}
    \label{def:convex_function}
    A function f is said \textbf{convex} if $\forall x, z \in \mathbb{R}^n$, $ \alpha \in [0,1] $ it holds $ \alpha f(x) + (1-\alpha )f(z) \geq f(\alpha x + (1- \alpha ) z)$
\end{definition}


\begin{definition}
    \label{def:differentiable_function}
    A function f is said \textbf{differentiable} in $\mathbb{R}^n$ at a point x if $\exists$ linear function $\phi(h) = <b,h> + f(x)$ s.t.
    $\lim_{||h|| \to 0} \frac{|f(x+h) - \phi (h)|}{||h||} = 0$
    
\end{definition}

\begin{definition}
    \label{def:LC}
    A function $f$ is said \textbf{Lipschitz continuous} on a domain if 
    $\exists L \in \mathbb{R}, L > 0$  s.t. $| f(x) - f(z)| \leq L|| x -z || \forall x,z  $
\end{definition}


\begin{definition}
    \label{def:Lsmooth}
    A function $f$ is said \textbf{L-Smooth} on a domain X if the gradient of f is Lipschitz continuous on X  
    $\exists L \in \mathbb{R}, L > 0$  s.t. $|| \nabla f(x) - \nabla f(z)| \leq L|| x -z || \forall x,z  $
\end{definition}



As defined above, we want to minimize the Loss function so we can see the objective function as in (Eq.\ref{eq:minproblem}), where $g(x)$ is the SSE while $h(x)$ is the penalization term (Ridge Regression).

\begin{equation}
\label{eq:minproblem}
\min_{x \in \mathbb{R}^n} f(x) = g(x) + h(x)
\end{equation}

To prove the convergence of ACM to a minimum we need to assume that: 

\begin{enumerate}

    \item $g(x)$ to be convex and differentiable as in \ref{def:convex_function} and \ref{def:differentiable_function}.
    \item $\nabla g$ to be Lipschitz continuous with constant L > 0  as stated in\ref{def:LC}.
    \item $h(x)$ to be convex as in \ref{def:convex_function}.
    \item update rule in \ref{eq:updateruleacm} can be evaluated correctly.
\end{enumerate}



So, if all these properties hold, we can use the Theorem in \ref{th:convergencerate} which gives us information on the rate at which the sequence $f(x^{(k)}) - f(x^*)$ goes to zero, where $x^{(k)}$ is the point at kth iteration and $x^*$ is the minimum of the function.

\begin{theorem}
\label{th:convergencerate}
Generalized Accelerated Gradient Methods with fixed stepsize $\alpha \leq \frac{1}{L}$ satisfies: \\
$f(x^{(k)}) - f(x^*) \leq \frac{2 || x^{(0)} - x^* ||^2}{\alpha (k+1)^2} = \epsilon$

\end{theorem}

So the convergence rate for ACM is $O(\frac{1}{k^2})$ and to get a precision such that $f(x_k) - f(x^*) \leq \epsilon$ we need $O(\frac{1}{\sqrt{\epsilon}})$ iterations.

%
%To prove the above convergence, therefore, we proceed as follows. $g$ is Lipschitz Continuous with constant $L > 0$ and stepsize $\alpha \leq \frac{1}{L}$, so it implies

%\[ g(x^{k+1}) \leq  g(y^{k+1}) + \nabla g(y^{k+1})^T \]


\newpage